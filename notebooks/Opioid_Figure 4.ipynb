{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f18899cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'contour_visualization'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcreate_mask_for_region\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcontour_visualization\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#import shap\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# read heatmap plots\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#import Heatmap_plots as hmp\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'contour_visualization'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from read_roi import read_roi_file\n",
    "import tifffile\n",
    "import matplotlib.pyplot as plt\n",
    "import create_mask_for_region\n",
    "from datetime import datetime\n",
    "from contour_visualization import *\n",
    "\n",
    "#import shap\n",
    "# read heatmap plots\n",
    "#import Heatmap_plots as hmp\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import tifffile as tiff\n",
    "\n",
    "def flatten_extend(matrix):\n",
    "    flat_list = []\n",
    "    for row in matrix:\n",
    "        flat_list.extend(row)\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9362011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set figure key\n",
    "figure_key = 'Figure4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f8233a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'figure_key' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m analysis_resultpath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(rootpath,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m analysis_root_figurepath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(rootpath,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfigure\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m analysis_figurepath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(analysis_root_figurepath,\u001b[43mfigure_key\u001b[49m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m [analysis_resultpath,analysis_root_figurepath,analysis_figurepath]:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(path):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'figure_key' is not defined"
     ]
    }
   ],
   "source": [
    "# set paths\n",
    "result_paths = r\"data/lsms/Opioid\"\n",
    "rootpath = r\"G:\\My Drive\\Opioid_whole_brain_manuscript\"\n",
    "metapath = os.path.join(rootpath,\"meta\")\n",
    "analysis_resultpath = os.path.join(rootpath,\"result\")\n",
    "analysis_root_figurepath = os.path.join(rootpath,\"figure\")\n",
    "analysis_figurepath = os.path.join(analysis_root_figurepath,figure_key)\n",
    "for path in [analysis_resultpath,analysis_root_figurepath,analysis_figurepath]:\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "# load meta info of the files\n",
    "metadf = pd.read_csv(os.path.join(metapath,\"OP_meta.csv\"),index_col= False)\n",
    "# temporary drop of A7 due to missing data\n",
    "metadf = metadf[metadf.ID != 'A7'].reset_index(drop = True)\n",
    "# load brain atlas to register\n",
    "atlas_df = pd.read_csv(r\"data/atlas/atlas_info_KimRef_FPbasedLabel_v4.0_brain_with_size_with_curated_with_cleaned_acronyms.csv\",index_col = False)\n",
    "metacolumns = ['id','acronym','parent_acronym','parent_id','structure_order']\n",
    "contour_img = tifffile.imread(r\"data/atlas/Kim_ref_adult_FP-label_v2.9_contour_map.tif\")\n",
    "# retrieve list of files\n",
    "fnames =  [f for f in metadf.fname.values if 'DONE' in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65a9975d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Saline', 'Acute_Morphine', 'Chronic_Morphine', 'Withdrawal_Morphine', 'Chronic_Morphine_21', 'Withdrawal_Morphine_21']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'metadf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(Conditions)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# subset the meta dataframe\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m metadf \u001b[38;5;241m=\u001b[39m \u001b[43mmetadf\u001b[49m[metadf\u001b[38;5;241m.\u001b[39mCondition\u001b[38;5;241m.\u001b[39misin(Conditions)]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'metadf' is not defined"
     ]
    }
   ],
   "source": [
    "#Conditions = metadf.Condition.unique()\n",
    "# Use only morphine related groups\n",
    "Conditions = ['Saline','Acute_Morphine','Chronic_Morphine','Withdrawal_Morphine','Chronic_Morphine_21','Withdrawal_Morphine_21',]\n",
    "print(Conditions)\n",
    "\n",
    "# subset the meta dataframe\n",
    "metadf = metadf[metadf.Condition.isin(Conditions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d6018e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from adjustText import adjust_text\n",
    "\n",
    "# Set matplotlib parameters for white text on transparent background\n",
    "plt.rcParams.update({\n",
    "    'figure.facecolor': 'none',  # Transparent figure background\n",
    "    'axes.facecolor': 'none',    # Transparent axes background\n",
    "    'axes.edgecolor': 'black',   # White axes edge color\n",
    "    'axes.labelcolor': 'black',  # White axis labels\n",
    "    'xtick.color': 'black',      # White tick labels\n",
    "    'ytick.color': 'black',      # White tick labels\n",
    "    'legend.facecolor': 'none',  # Transparent legend background\n",
    "    'legend.edgecolor': 'none',  # Transparent legend edgecolor\n",
    "    'text.color': 'black',       # White text color\n",
    "    'font.family':'Arial',\n",
    "    'pdf.fonttype':42,\n",
    "    'ps.fonttype':42,\n",
    "   \n",
    "})\n",
    "#important for text to be detected when importing saved figures into illustrator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a6ffcc4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(analysis_resultpath,\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcurated_acronym.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[1;32m----> 2\u001b[0m     curated_acronyms \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241m.\u001b[39mload(handle)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(analysis_resultpath,\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mancestor_curated_acronym.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[0;32m      5\u001b[0m     ancestor_curated_acronyms \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(handle,)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(analysis_resultpath,f'curated_acronym.pickle'), 'rb') as handle:\n",
    "    curated_acronyms = pickle.load(handle)\n",
    "\n",
    "with open(os.path.join(analysis_resultpath,f'ancestor_curated_acronym.pickle'), 'rb') as handle:\n",
    "    ancestor_curated_acronyms = pickle.load(handle,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7596e453",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_depth = 4\n",
    "# set heatmap variables\n",
    "vmin= -5\n",
    "vmax = 10\n",
    "\n",
    "#Conditions = metadf.Condition.unique()\n",
    "# Use only morphine related groups\n",
    "Conditions = ['Saline','Acute_Morphine','Chronic_Morphine','Withdrawal_Morphine','Chronic_Morphine_21','Withdrawal_Morphine_21',]\n",
    "print(Conditions)\n",
    "#Condition_figure_name = ['Saline','Acute','Chronic 1 day','W.D. 1 day','Chronic 21 days','W.D. 21 days'] # changed this to betterones\n",
    "Condition_figure_name = ['Saline','Acute','Chronic','Early WD','Re-exposure','Late WD'] \n",
    "Condition_color = ['gray','lime','orange','cyan','blue','purple']\n",
    "# subset the meta dataframe\n",
    "metadf = metadf[metadf.Condition.isin(Conditions)]\n",
    "\n",
    "# load and subset dataframes\n",
    "pivot_heatmap_df = pd.read_csv(os.path.join(analysis_resultpath,'long_pivoted_heatmap_df_with_normalized_density.csv'),index_col = 0)\n",
    "pivot_heatmap_df = pivot_heatmap_df[metadf[metadf.Condition.isin(Conditions)]['ID'].values]\n",
    "merge_df  = pd.read_csv(os.path.join(analysis_resultpath,'Ex_639_Ch2_stitched_long_merge_Annotated_counts_with_leaf_with_density_with_normalized_density.csv'),index_col = 0)\n",
    "merge_df = merge_df[merge_df.Condition.isin(Conditions)]\n",
    "# Load the acronyms for plotting\n",
    "with open(os.path.join(analysis_resultpath,f'curated_acronym.pickle'), 'rb') as handle:\n",
    "    curated_acronyms = pickle.load(handle)\n",
    "\n",
    "with open(os.path.join(analysis_resultpath,f'ancestor_curated_acronym.pickle'), 'rb') as handle:\n",
    "    ancestor_curated_acronyms = pickle.load(handle,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cdf0ec",
   "metadata": {},
   "source": [
    "remove HB and CBL from the list of ancestores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "930fadc7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'atlas_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m unique_ancestor_curated_acronyms \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIsocortex\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOLF\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHPF\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCTXsp\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSTR\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPAL\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTH\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHY\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMB\u001b[39m\u001b[38;5;124m'\u001b[39m,]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#unique_ancestor_curated_acronyms = ['Isocortex','OLF','HPF','CTXsp','STR','PAL','TH','HY','MB','HB','CBL']\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# get a list of idx for the ancestors\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m ancestor_names \u001b[38;5;241m=\u001b[39m [\u001b[43matlas_df\u001b[49m\u001b[38;5;241m.\u001b[39mloc[atlas_df\u001b[38;5;241m.\u001b[39macronym \u001b[38;5;241m==\u001b[39m f,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m unique_ancestor_curated_acronyms]\n\u001b[0;32m      7\u001b[0m ancestor_idxs \u001b[38;5;241m=\u001b[39m [atlas_df\u001b[38;5;241m.\u001b[39mloc[atlas_df\u001b[38;5;241m.\u001b[39macronym \u001b[38;5;241m==\u001b[39m f,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m unique_ancestor_curated_acronyms]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03mcurated_acronyms = []\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03mancestor_curated_acronyms = []\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03mwith open(os.path.join(analysis_resultpath,f'ancestor_curated_acronym.pickle'), 'wb') as handle:\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03m    pickle.dump(ancestor_curated_acronyms,handle,)'''\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'atlas_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Update the ancestor curated acronyms so it matches the tree devisions\n",
    "unique_ancestor_curated_acronyms = ['Isocortex','OLF','HPF','CTXsp','STR','PAL','TH','HY','MB',]\n",
    "#unique_ancestor_curated_acronyms = ['Isocortex','OLF','HPF','CTXsp','STR','PAL','TH','HY','MB','HB','CBL']\n",
    "\n",
    "# get a list of idx for the ancestors\n",
    "ancestor_names = [atlas_df.loc[atlas_df.acronym == f,'name'].values[0] for f in unique_ancestor_curated_acronyms]\n",
    "ancestor_idxs = [atlas_df.loc[atlas_df.acronym == f,'id'].values[0] for f in unique_ancestor_curated_acronyms]\n",
    "'''\n",
    "curated_acronyms = []\n",
    "ancestor_curated_acronyms = []\n",
    "for idx,i in enumerate(ancestor_idxs):\n",
    "    create_mask_for_region.get_subregions(atlas_df,i,return_original = True)\n",
    "    tdf = create_mask_for_region.get_subregions(atlas_df,i,return_original = True)\n",
    "    curated_acronyms += list(tdf[tdf.Curated_list].acronym)\n",
    "    ancestor_curated_acronyms += [unique_ancestor_curated_acronyms[idx]]*tdf[tdf.Curated_list].shape[0]\n",
    "\n",
    "# save the new list of acronyms\n",
    "with open(os.path.join(analysis_resultpath,f'curated_acronym.pickle'), 'wb') as handle:\n",
    "    pickle.dump(curated_acronyms,handle,)\n",
    "\n",
    "with open(os.path.join(analysis_resultpath,f'ancestor_curated_acronym.pickle'), 'wb') as handle:\n",
    "    pickle.dump(ancestor_curated_acronyms,handle,)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58d1dca",
   "metadata": {},
   "source": [
    "remove CBL and MB subtree from the atlas file and the merge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9c4d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove CBL and MB subtree from the data. These regions had bad registration quality and low interest\n",
    "remove_ancestor_ids = atlas_df[(atlas_df.acronym == 'HB') | (atlas_df.acronym == 'CBL')]['id'].values\n",
    "remove_df = pd.concat([create_mask_for_region.get_subregions(atlas_df,idx,return_original = True) for idx in remove_ancestor_ids],axis = 0)\n",
    "sub_atlas_df = atlas_df.set_index(['id']).drop(remove_df['id'].values)\n",
    "merge_df = merge_df[merge_df.acronym.isin(sub_atlas_df.acronym.unique())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed09adf4",
   "metadata": {},
   "source": [
    "set up the metacolumns to be compatible for GLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e43375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load meta info of the files\n",
    "metadf['age'] = [(datetime.strptime( pday, '%m/%d/%Y') - datetime.strptime( dob, '%m/%d/%Y')).days for pday,dob in metadf.loc[:,['Date_Perfusion','DOB']].values]\n",
    "atlasmeta = merge_df.reset_index().loc[merge_df.reset_index().ID =='A1',['id','parent_id','acronym','name','parent_acronym']]\n",
    "\n",
    "#metaexog = metadf[['Condition','Sex','BW','age','Staining_Batch']]\n",
    "#metacolumns = ['Saline','Acute_Morphine','Chronic_Morphine','Sex_d','Batch_d']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac2b0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change categorical values to dummy chategories dtypes\n",
    "sex_category = pd.CategoricalDtype(categories=['F', 'M'], ordered=False)\n",
    "condition_category = pd.CategoricalDtype(categories=Conditions, ordered=True)\n",
    "batch_category = pd.CategoricalDtype(categories=[1,2,3,4], ordered=False)\n",
    "\n",
    "merge_df['Sex']             = merge_df['Sex'].astype(sex_category)\n",
    "merge_df['Condition']       = merge_df['Condition'].astype(condition_category)\n",
    "\n",
    "merge_df['Staining_Batch']  = merge_df['Staining_Batch'].astype(batch_category)\n",
    "\n",
    "# create dummy cats\n",
    "condition_dummies           = pd.get_dummies(merge_df['Condition'])\n",
    "sex_dummies                 = pd.get_dummies(merge_df['Sex']).loc[:,['F']].rename(columns = {'F':'Sex_d'}) # female 1\n",
    "batch_dummies = pd.get_dummies(merge_df['Staining_Batch'])\n",
    "batch_dummies.columns = [f'Batch_{c}_d' for c in range(4)]\n",
    "\n",
    "merge_df                    = pd.concat([merge_df,condition_dummies,sex_dummies,batch_dummies],axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76febff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add flags for conditions\n",
    "#merge_df = pd.merge(merge_df,metadf[['ID','Acute_flag','Chronic_flag','Spontaneous_flag']],left_on = 'ID',right_on = 'ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c523c581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate effect size raw\n",
    "raw_effect_size_df = merge_df[['acronym','Condition','density']].groupby(['acronym','Condition',]).mean().reset_index()\\\n",
    ".pivot(columns = 'Condition',index = 'acronym',values = 'density')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2de646a",
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas_df.loc[atlas_df.acronym.isin(curated_acronyms),['acronym','name']].to_csv(os.path.join(metapath,'clean_curated_acronyms.csv'),index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4103f27e",
   "metadata": {},
   "source": [
    "Prepare heatmaps into a dictionary format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cee0813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read an annotated atlas file\n",
    "atlas_img = tifffile.imread(r\"data/atlas/Kim_ref_adult_FP-label_v4.0.tif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693d45d6",
   "metadata": {},
   "source": [
    "# Figure 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74b1981",
   "metadata": {},
   "source": [
    "## Figure 4B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdb0529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def set_transparency(rgba_img, mask):\n",
    "    \"\"\"\n",
    "    Applies a transparency mask to an existing RGBA image.\n",
    "\n",
    "    Parameters:\n",
    "    - rgba_img: np.ndarray of shape (H, W, 4), dtype uint8\n",
    "        The input RGBA image.\n",
    "    - mask: np.ndarray of shape (H, W), dtype bool\n",
    "        Boolean mask where True means the pixel should be transparent.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray of shape (H, W, 4), modified RGBA image.\n",
    "    \"\"\"\n",
    "    if rgba_img.shape[-1] != 4:\n",
    "        raise ValueError(\"Input image must be RGBA (shape must be H x W x 4).\")\n",
    "    if rgba_img.shape[:2] != mask.shape:\n",
    "        raise ValueError(\"Mask shape must match image height and width.\")\n",
    "\n",
    "    # Copy to avoid modifying the original\n",
    "    result = rgba_img.copy()\n",
    "    result[mask, 3] = 0  # Set alpha to 0 (transparent) where mask is True\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35247254",
   "metadata": {},
   "outputs": [],
   "source": [
    "pannel_key = 'B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf957250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot every 10 zplanes\n",
    "# slice for visualization\n",
    "imy_slice = slice(25,425)\n",
    "imx_slice = slice(50,600)\n",
    "\n",
    "# pre selected zplanes\n",
    "curated_zplanes = [84,104,117,153,186,220]\n",
    "\n",
    "chronic_heatmap = np.load(os.path.join(r\"data/opioid_cfos/result\",f'Chronic_Morphine_betas.npy'))\n",
    "withdrawal_heatmap = np.load(os.path.join(r\"data/opioid_cfos/result\",f'Withdrawal_Morphine_betas.npy'))\n",
    "\n",
    "# subtract the withdrawal heatmap from the chronic heatmap\n",
    "theatmap = chronic_heatmap - withdrawal_heatmap\n",
    "\n",
    "fig,axs = plt.subplots(1,len(curated_zplanes),figsize = (3*len(curated_zplanes),3),sharey = True)\n",
    "fig.subplots_adjust(wspace=0.25, hspace=0.3)\n",
    "\n",
    "for idx,ax in enumerate(axs):\n",
    "    formatted_idx = f\"{curated_zplanes[idx]:04}\" \n",
    "    \n",
    "    __,overlayed_image = overlap_contour(theatmap,contour_img,\\\n",
    "    cmin =  -15, cmax = 15,\\\n",
    "    outputpath = None)\n",
    "    trans_img = set_transparency(overlayed_image[curated_zplanes[idx],:,:], (atlas_img==0)[curated_zplanes[idx],:,:])\n",
    "    #ax.figure(figsize=(10, 10))\n",
    "    ax.imshow(trans_img[imy_slice, imx_slice,])\n",
    "    ax.axis('off')\n",
    "    ax.set_title('')\n",
    "    #ax.set_ylabel(condition,color = 'black')\n",
    "fig.savefig(os.path.join(analysis_figurepath,f'{figure_key}{pannel_key}.png'),bbox_inches='tight',dpi = 1024)\n",
    "fig.savefig(os.path.join(analysis_figurepath,f'{figure_key}{pannel_key}.pdf'),bbox_inches='tight',dpi = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166e2b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def set_transparency(rgba_img, mask):\n",
    "    \"\"\"\n",
    "    Applies a transparency mask to an existing RGBA image.\n",
    "\n",
    "    Parameters:\n",
    "    - rgba_img: np.ndarray of shape (H, W, 4), dtype uint8\n",
    "        The input RGBA image.\n",
    "    - mask: np.ndarray of shape (H, W), dtype bool\n",
    "        Boolean mask where True means the pixel should be transparent.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray of shape (H, W, 4), modified RGBA image.\n",
    "    \"\"\"\n",
    "    if rgba_img.shape[-1] != 4:\n",
    "        raise ValueError(\"Input image must be RGBA (shape must be H x W x 4).\")\n",
    "    if rgba_img.shape[:2] != mask.shape:\n",
    "        raise ValueError(\"Mask shape must match image height and width.\")\n",
    "\n",
    "    # Copy to avoid modifying the original\n",
    "    result = rgba_img.copy()\n",
    "    result[mask, 3] = 0  # Set alpha to 0 (transparent) where mask is True\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8ea3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot every .3 mm\n",
    "# slice for visualization\n",
    "imy_slice = slice(25,425)\n",
    "imx_slice = slice(50,600)\n",
    "\n",
    "# pre selected zplanes\n",
    "curated_zplanes = np.arange(79,229,6) # every .3 mm = 6 zplanes x 50 um \n",
    "\n",
    "\n",
    "Chronic_Morphine_heatmap = np.load(os.path.join(r\"data/opioid_cfos/result\",f'Chronic_Morphine_betas.npy'))\n",
    "Withdrawal_Morphine_heatmap = np.load(os.path.join(r\"data/opioid_cfos/result\",f'Withdrawal_Morphine_betas.npy'))\n",
    "\n",
    "# subtract the withdrawal heatmap from the chronic heatmap\n",
    "theatmap = Chronic_Morphine_heatmap - Withdrawal_Morphine_heatmap\n",
    "__,overlayed_image = overlap_contour(theatmap,contour_img,\\\n",
    "cmin =  -15, cmax = 15,\\\n",
    "outputpath = None)\n",
    "\n",
    "fig,axs = plt.subplots(len(curated_zplanes)//6,7,figsize = (3*3,len(curated_zplanes)//7),sharey = True)\n",
    "fig.subplots_adjust(wspace=0., hspace=0.)\n",
    "\n",
    "for idx,zplane in enumerate(curated_zplanes):\n",
    "    if idx < 7:\n",
    "        rows = 0\n",
    "        columns = idx%7\n",
    "    elif idx < 13:\n",
    "        rows = 1\n",
    "        columns = (idx + rows-1)%7\n",
    "    elif idx < 19:\n",
    "        rows = 2\n",
    "        columns = (idx + rows-1)%7\n",
    "    elif idx < 25:\n",
    "        rows = 3\n",
    "        columns = (idx + rows-1)%7\n",
    "\n",
    "    ax = axs[rows,columns]\n",
    "    formatted_idx = f\"{zplane:04}\" \n",
    "    trans_img = set_transparency(overlayed_image[zplane,:,:], (atlas_img==0)[zplane,:,:])\n",
    "    ax.imshow(trans_img[imy_slice,imx_slice])\n",
    "    ax.axis('off')\n",
    "    ax.set_title('')\n",
    "    #ax.set_ylabel(condition,color = 'black')\n",
    "fig.savefig(os.path.join(analysis_figurepath,f'{figure_key}{pannel_key}_03mm.png'),bbox_inches='tight',dpi = 1024)\n",
    "fig.savefig(os.path.join(analysis_figurepath,f'{figure_key}{pannel_key}_03mm.pdf'),bbox_inches='tight',dpi = 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9a7a1b",
   "metadata": {},
   "source": [
    "## Preparation for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05513c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set conditions to compare\n",
    "#sub_conditions  = ['Saline','Acute_Morphine']\n",
    "sub_conditions  = ['Saline','Acute_Morphine','Chronic_Morphine','Withdrawal_Morphine','Chronic_Morphine_21','Withdrawal_Morphine_21',]\n",
    "sub_IDs         = metadf[metadf.Condition.isin(sub_conditions)].ID.values\n",
    "sub_merge_df    = merge_df[merge_df.Condition.isin(sub_conditions)]\n",
    "#sub_pivot_df    = pivot_heatmap_df[sub_IDs]\n",
    "sub_pivot_df    = merge_df.pivot(columns = 'ID',index= 'acronym',values = 'density')[sub_IDs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9521df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the results from the treeFDR f\n",
    "TreeFDRF_df = pd.read_csv(os.path.join(analysis_resultpath,f'TreeFDRF_pvalue_Figure2_C_glm_stat_df_no_batch.csv'),index_col = False)\n",
    "# add the cleaned acronym names to the TreeFDR dataframe\n",
    "TreeFDRF_df = TreeFDRF_df.merge(atlas_df[['acronym','cleaned_acronym']],left_on = 'acronym',right_on = 'acronym')\n",
    "# get the rejected acronyms and print\n",
    "rejected_acronyms = TreeFDRF_df[(TreeFDRF_df.acronym.isin(curated_acronyms))&(TreeFDRF_df.rejected == True)].acronym.values\n",
    "print(len(rejected_acronyms), ' rejected acronyms')\n",
    "rejected_acronyms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d73c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap.umap_\n",
    "import umap.plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea858033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset data to non saline subjects\n",
    "# the chronic and withdrawal conditions were used for the clustering. \n",
    "# Subset the dataframe to only rejected_acronyms that were defined from the TreeBH analysis in Figure 2.\n",
    "\n",
    "average_key = False\n",
    "subset_key = True\n",
    "subset_conditions = ['Chronic_Morphine','Withdrawal_Morphine']\n",
    "\n",
    "# store the results in a new dataframe\n",
    "umap_data = sub_pivot_df.loc[rejected_acronyms,metadf[metadf.Condition.isin(subset_conditions)]['ID']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d2cf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data frame using a sklearn package\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scale_key = True\n",
    "\n",
    "# scale and store the dataframe again\n",
    "data = scaler.fit_transform(umap_data.to_numpy().T)\n",
    "data = pd.DataFrame(data.T, columns = umap_data.columns,index = umap_data.index)\n",
    "'''teffect_size_df = effect_size_df.set_index('acronym').loc[rejected_acronyms,Conditions[1:]]\n",
    "scaled_effect_size_df = scaler.fit_transform(teffect_size_df.to_numpy().T)\n",
    "scaled_effect_size_df = pd.DataFrame(scaled_effect_size_df.T, columns = teffect_size_df.columns,index = teffect_size_df.index)\n",
    "'''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8bf4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try plotting the heatamap\n",
    "sns.heatmap(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21c6444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to determine variables\n",
    "def draw_umap(n_neighbors=15, min_dist=0.1, n_components=2, metric='euclidean', title=''):\n",
    "    fit = umap.UMAP(\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=min_dist,\n",
    "        n_components=n_components,\n",
    "        metric=metric\n",
    "    )\n",
    "    u = fit.fit_transform(data);\n",
    "    fig = plt.figure()\n",
    "    if n_components == 1:\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.scatter(u[:,0], range(len(u)), )\n",
    "    if n_components == 2:\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.scatter(u[:,0], u[:,1], )\n",
    "    if n_components == 3:\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.scatter(u[:,0], u[:,1], u[:,2],  s=100)\n",
    "    plt.title(title, fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dfd844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the number of neighbors\n",
    "n_neighbors = 3\n",
    "# determine the minimum distance\n",
    "min_dist = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dab10ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create UMAP embedding\n",
    "mapper = umap.UMAP(n_components = 2,n_neighbors=n_neighbors, min_dist=min_dist,random_state = 45).fit(data)\n",
    "\n",
    "# Extract the embedding coordinates\n",
    "embedding = mapper.embedding_\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "# Plot the points with larger size\n",
    "sc = ax.scatter(embedding[:, 0], embedding[:, 1], s=50, alpha=0.5)  # Adjust 's' for larger points\n",
    "\n",
    "texts = []\n",
    "# Annotate each point with the corresponding acronym\n",
    "for i, acronym in enumerate(data.index):\n",
    "    texts.append(ax.text(embedding[i, 0], embedding[i, 1], acronym, fontsize=9, ha='right'))\n",
    "\n",
    "'''spcific_texts = []\n",
    "# for specific genes\n",
    "for i, gene in enumerate(tdf[np.sum(tdf[[condition1,condition2]]>std,axis = 1)>0].index):\n",
    "    gene_symbol = atlas_df.set_index('acronym').loc[gene,'cleaned_acronym']\n",
    "    if not gene in interesting_regions:\n",
    "        continue\n",
    "    spcific_texts.append(axs.text(\n",
    "        tdf.loc[gene,condition1],tdf.loc[gene,condition2],\n",
    "                            gene_symbol, fontsize=12, alpha=0.75,color='black'))    '''\n",
    "\n",
    "adjust_text(texts,ax = ax, arrowprops=dict(color='black', lw=0.5,alpha = 0.75))\n",
    "\n",
    "# Show the plot\n",
    "plt.title(f\"UMAP projection with Brain Region Acronyms: {idx}\")\n",
    "plt.xlabel(\"UMAP 1\")\n",
    "plt.ylabel(\"UMAP 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc29f944",
   "metadata": {},
   "source": [
    "## Figure E"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f907e3ec",
   "metadata": {},
   "source": [
    "Plot the hdbscan linkage tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7a4007",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "pannel_key = 'E'\n",
    "# Assuming you have already fitted your data with HDBSCAN\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=3,)\n",
    "clusterer.fit(embedding)\n",
    "\n",
    "# Plot the single linkage tree to confirm the number of clusters\n",
    "fig,axs = plt.subplots(1,1,figsize= (4,2))\n",
    "axs = clusterer.single_linkage_tree_.plot(cmap='viridis', colorbar=True)\n",
    "# Save the figure\n",
    "fig.savefig(os.path.join(analysis_figurepath, f'{figure_key}{pannel_key}_hdcbscan_tree.tif'), bbox_inches='tight', dpi=216)\n",
    "fig.savefig(os.path.join(analysis_figurepath, f'{figure_key}{pannel_key}_hdcbscan_tree.pdf'), bbox_inches='tight', dpi=216)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf815ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866aa617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the ideal clusters (e.g., 3 clusters)\n",
    "labels = clusterer.single_linkage_tree_.get_clusters(2, min_cluster_size=3)\n",
    "\n",
    "# rearrange the labels to match the original order\n",
    "original_label_order = [2,3,1,0,4,5]\n",
    "set_labels = [1,2,3,4,5,6]\n",
    "labels = [set_labels[np.where(np.array(original_label_order) == i)[0][0]] for i in labels]\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349479cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the results\n",
    "np.save(os.path.join(analysis_resultpath,f'{figure_key}{pannel_key}_umap_mapper_variables.npy'),mapper)\n",
    "\n",
    "# write the UMAP embedding\n",
    "embedding_df = pd.DataFrame(embedding,columns = ['UMAP1', 'UMAP2'],index = data.index)\n",
    "# add labels to the dataframe and write\n",
    "embedding_df['label'] = labels\n",
    "# add results from the tree analysis\n",
    "embedding_df['rejected'] = [f in rejected_acronyms for f in embedding_df.index]\n",
    "embedding_df.to_csv(os.path.join(analysis_resultpath,f'{figure_key}{pannel_key}_umap_embedding.csv'),index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99173749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read results umap\n",
    "mapper_variables =  np.load(os.path.join(analysis_resultpath,f'{figure_key}{pannel_key}_umap_mapper_variables.npy'),allow_pickle=True)\n",
    "embedding_df = pd.read_csv(os.path.join(analysis_resultpath,f'{figure_key}{pannel_key}_umap_embedding.csv'),index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50a39ad",
   "metadata": {},
   "source": [
    "## Figure C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a9cbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pannel_key = 'C'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2db6158",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered = (embedding_df['label'] >= 0)\n",
    "rejected = (embedding_df['rejected'] == True)\n",
    "\n",
    "fig,axs = plt.subplots(1,1,figsize = (4.5,4.5))\n",
    "scatter = plt.scatter(embedding[clustered & rejected, 0],\n",
    "            embedding[clustered & rejected, 1],\n",
    "            c=embedding_df['label'][clustered & rejected],\n",
    "            s=60,edgecolor = 'k',\n",
    "            cmap='Set1');\n",
    "# Adding a legend for the K-means clusters\n",
    "axs.legend(*scatter.legend_elements(), title = 'Cluster',)\n",
    "\n",
    "# plot annotations for specific brain regions that are in our interest\n",
    "curated_texts = ['VTA','VTg','PT','MD','PVT','AcbC','AcbSh','LS','O','ASt','La',\n",
    "                 'CPce','CPc','Ce','ST','BL','MBO','DP','Pl','DS','CPre','STr','PrL','DI','AI','IL','Cg','SPTg','Cl'] + \\\n",
    "                    ['A24a (IL)','Au1','RLi ','PrEW','A24 (Cg)','Cl', 'A32 (PrL)',   'VTA', 'AcbC', 'AcbSh','La','IPAC','PrG','Ce', 'ST', ]\n",
    "curated_texts = np.unique(curated_texts)\n",
    "texts = []\n",
    "# Annotate each point with the corresponding acronym\n",
    "for i, acronym in enumerate(data.index):\n",
    "    clean_acronym = atlas_df[atlas_df.acronym == acronym].cleaned_acronym.values[0]\n",
    "    tname = atlas_df[atlas_df.acronym == acronym].name.values[0]\n",
    "    #print(acronym,tname)\n",
    "    if not clean_acronym in curated_texts:\n",
    "        continue\n",
    "    if acronym in rejected_acronyms:\n",
    "        texts.append(axs.text(embedding[i, 0], embedding[i, 1], clean_acronym, fontsize=10, ha='right'))\n",
    "\n",
    "\n",
    "adjust_text(texts,ax = axs, expand=(2, 2),force_text=(0.25,0.25),arrowprops=dict(color='gray', lw=1,alpha = 0.75))\n",
    "axs.set_ylabel('UMAP1',fontsize = 12)\n",
    "axs.set_xlabel('UMAP2',fontsize = 12)\n",
    "\n",
    "fig.savefig(os.path.join(analysis_figurepath,f'{figure_key}{pannel_key}.png'),bbox_inches ='tight',dpi = 216)\n",
    "fig.savefig(os.path.join(analysis_figurepath,f'{figure_key}{pannel_key}.pdf'),bbox_inches ='tight',dpi = 216)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673962fc",
   "metadata": {},
   "source": [
    "## Figure D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7cd499",
   "metadata": {},
   "source": [
    "Plot the average normalized density on top of the umap for each condition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ecfadf",
   "metadata": {},
   "source": [
    "### preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615447c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# effect size data frame\n",
    "# this will be using normalized_density, which density values normalized to the mean saline data. \n",
    "effect_size_df = merge_df[['acronym','Condition','normalized_density']].groupby(['acronym','Condition']).mean().reset_index().pivot(index = 'acronym',columns = 'Condition',values = 'normalized_density').reset_index()\n",
    "teffect_size_df = effect_size_df.set_index('acronym').loc[rejected_acronyms,Conditions[1:]]\n",
    "scaled_effect_size_df = scaler.fit_transform(teffect_size_df.to_numpy().T)\n",
    "scaled_effect_size_df = pd.DataFrame(scaled_effect_size_df.T, columns = teffect_size_df.columns,index = teffect_size_df.index).reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549ae485",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ea1c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "pannel_key = 'D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca17946b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin = -1.5\n",
    "vmax = 1.5\n",
    "fig,axs = plt.subplots(2,3,figsize=(len(Conditions[1:])*0.7*3, 3*2),)\n",
    "\n",
    "\n",
    "for cidx, Condition in enumerate(Conditions[1:]):\n",
    "    ordered_acronym = effect_size_df.set_index('acronym').loc[rejected_acronyms, :].sort_values(by=Condition).index\n",
    "    ax = axs[(cidx//3),(cidx%3)]\n",
    "    # Plotting the UMAP embedding with K-means clusters\n",
    "    scatter = ax.scatter(embedding_df.loc[ordered_acronym,'UMAP1'], embedding_df.loc[ordered_acronym,'UMAP2'],\n",
    "                         c=scaled_effect_size_df.set_index('acronym').loc[ordered_acronym, Condition], s=40,\n",
    "                         #c=data.loc[rejected_acronyms, Condition], s=100,\n",
    "                          alpha=0.7, vmin=vmin, vmax=vmax,cmap = 'coolwarm')\n",
    "\n",
    "    # Adding a color bar for the effect sizes\n",
    "    if cidx == len(scaled_effect_size_df.columns[[1,4]])-1:\n",
    "        colorbar = plt.colorbar(scatter, ax=axs, fraction=0.046, pad=0.04)\n",
    "        colorbar.set_label('Scaled Effect Size', rotation=-90)\n",
    "\n",
    "    # Annotating the points with brain region names\n",
    "    #texts = []\n",
    "    #for i, brain_region in enumerate(embedding_df.index):\n",
    "    #    texts.append(ax.text(embedding_df['UMAP1'][i], embedding_df['UMAP2'][i],\n",
    "    #                         brain_region, fontsize=10, alpha=0.75))\n",
    "\n",
    "\n",
    "    fCondition = Condition_figure_name[cidx+1].replace(\"_\", ' ')\n",
    "    ax.set_title(f'{fCondition}')\n",
    "    ax.set_xlabel('UMAP1',fontsize = 12)\n",
    "    ax.set_ylabel('UMAP2',fontsize = 12)\n",
    "# remove all the spines\n",
    "for ax in axs.flatten():\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "# Save the figure\n",
    "fig.savefig(os.path.join(analysis_figurepath, f'{figure_key}{pannel_key}.tif'), bbox_inches='tight', dpi=216)\n",
    "fig.savefig(os.path.join(analysis_figurepath, f'{figure_key}{pannel_key}.pdf'), bbox_inches='tight', dpi=216)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d2307e",
   "metadata": {},
   "source": [
    "## Figure F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9206ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pannel_key = 'F'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca76cdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add label info to the data frame\n",
    "data = data.join(embedding_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ce8575",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create a stacked version of the heatmap dataframe\n",
    "stack_data = data.reset_index().set_index(['acronym','label','UMAP1','UMAP2','rejected']).stack().reset_index().rename(columns = {'level_5':'ID',0:'normalized_density'})\n",
    "stack_data = stack_data.merge(metadf[['ID','Condition']],left_on = 'ID',right_on = 'ID')\n",
    "\n",
    "fig,axs = plt.subplots(1,1,figsize= (2,1.5))\n",
    "\n",
    "sns.stripplot(data = stack_data[stack_data.Condition.isin(['Chronic_Morphine','Withdrawal_Morphine'])],\n",
    "              hue = 'Condition',y = 'normalized_density',x = 'label',dodge = True,order = set_labels,\n",
    "        hue_order = Conditions[2:4],ax = axs,palette = Condition_color[2:4],alpha = 0.2,size = 1)\n",
    "sns.pointplot(data = stack_data[stack_data.Condition.isin(['Chronic_Morphine','Withdrawal_Morphine'])],\n",
    "              hue = 'Condition',y = 'normalized_density',x = 'label',order = set_labels,\n",
    "              dodge = .6-0.6/4,errorbar = (\"ci\",95),\\\n",
    "        hue_order = Conditions[2:4],ax = axs,palette = Condition_color[2:4],\n",
    "markers=\"o\", markersize=5, linestyle=\"none\",linewidth = 1)  \n",
    "\n",
    "sns.despine()\n",
    "#axs.set_xticklabels([str(idx + 1) for idx in range(4)])\n",
    "axs.set_xlabel('Cluster')\n",
    "# remove legend\n",
    "axs.get_legend().remove()\n",
    "axs.set_ylabel('Scaled density',fontsize = 12)\n",
    "from statannotations.Annotator import Annotator\n",
    "pairs = [((c,\"Chronic_Morphine\"),(c,\"Withdrawal_Morphine\")) for c in set_labels]\n",
    "\n",
    "annotator = Annotator(axs, pairs, data=stack_data[stack_data.Condition.isin(['Chronic_Morphine','Withdrawal_Morphine'])],\n",
    "                     y='normalized_density', x='label',order = set_labels,hue = 'Condition',hue_order = ['Chronic_Morphine','Withdrawal_Morphine'])\n",
    "annotator.configure(test='t-test_ind', text_format='star', loc='outside',)\n",
    "annotator.configure(comparisons_correction=\"BH\", correction_format=\"replace\")\n",
    "annotator.apply_and_annotate()\n",
    "\n",
    "fig.savefig(os.path.join(analysis_figurepath,f'{figure_key}{pannel_key}.png'),bbox_inches = 'tight',dpi = 216)\n",
    "fig.savefig(os.path.join(analysis_figurepath,f'{figure_key}{pannel_key}.pdf'),bbox_inches = 'tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb129df",
   "metadata": {},
   "source": [
    "## Figure G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48283941",
   "metadata": {},
   "outputs": [],
   "source": [
    "pannel_key = 'G'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a07d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format the data so it can be plotted\n",
    "sub_merge_df    = merge_df[merge_df.Condition.isin(sub_conditions)]\n",
    "sub_merge_df = sub_merge_df.merge(data.stack().reset_index(drop = False).rename(columns = {'level_1':'ID',0:'scaled_density'}),\n",
    "left_on = ['acronym','ID'],\n",
    "right_on = ['acronym','ID'])\n",
    "sub_merge_df.Condition = sub_merge_df.Condition.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b76defa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statannotations.Annotator import Annotator\n",
    "\n",
    "# create a barplot with a swarmplot overlapped\n",
    "sorted_acronyms = ['A24a (IL)','Au1','RLi ','PrEW','A24 (Cg)','Cl', 'A32 (PrL)',   'VTA', 'AcbC', 'AcbSh',\n",
    "       'La','IPAC','PrG','Ce', 'ST', ]\n",
    "non_sorted_acronyms = [f for f in rejected_acronyms if f not in sorted_acronyms]\n",
    "pairs = [((c,\"Chronic_Morphine\"),(c,\"Withdrawal_Morphine\")) for c in sorted_acronyms + non_sorted_acronyms]\n",
    "\n",
    "sub_conditions = ['Chronic_Morphine','Withdrawal_Morphine']\n",
    "\n",
    "sorted_cleaned_acronyms = atlas_df.set_index('acronym').loc[sorted_acronyms + non_sorted_acronyms,'cleaned_acronym'].values\n",
    "tdata = sub_merge_df[(sub_merge_df.Condition.isin(sub_conditions))&(sub_merge_df.acronym.isin(sorted_acronyms + non_sorted_acronyms))]\n",
    "tdata.scaled_density = tdata.scaled_density.astype('float64')\n",
    "fig,axs = plt.subplots(1,1,figsize = (len(sorted_acronyms + non_sorted_acronyms)//3.2,1.5))\n",
    "\n",
    "sub_merge_df.Condition = sub_merge_df.Condition.astype('str')\n",
    "sns.stripplot(data = tdata,\n",
    "            hue = 'Condition',y = 'scaled_density',x = 'acronym',dodge = True,\n",
    "            order = sorted_acronyms + non_sorted_acronyms,\\\n",
    "        hue_order = Conditions[2:4],ax = axs,palette = Condition_color[2:4],alpha = 0.25,size = 2)\n",
    "sns.pointplot(tdata,\n",
    "            hue = 'Condition',y = 'scaled_density',x = 'acronym',order = sorted_acronyms + non_sorted_acronyms,\n",
    "            dodge = .5-.5/len(sorted_acronyms + non_sorted_acronyms),\\\n",
    "        hue_order = Conditions[2:4],ax = axs,palette = Condition_color[2:4],\n",
    "markers=\"o\", markersize=3, linestyle=\"none\",linewidth = 0.5)  \n",
    "sns.despine()\n",
    "axs.set_xticks(range(len(sorted_cleaned_acronyms)))\n",
    "axs.set_xticklabels(sorted_cleaned_acronyms,fontsize = 11,rotation = -45)\n",
    "# remove legend\n",
    "axs.get_legend().remove()\n",
    "axs.set_xlabel('')\n",
    "axs.set_ylabel('Scaled density',fontsize = 12)\n",
    "\n",
    "\n",
    "pairs = [((c,\"Chronic_Morphine\"),(c,\"Withdrawal_Morphine\")) for c in sorted_acronyms + non_sorted_acronyms]\n",
    "\n",
    "annotator = Annotator(axs, pairs, data=tdata,\n",
    "                     y='scaled_density', x='acronym',hue = 'Condition',\n",
    "                     hue_order = Conditions[2:4],order = sorted_acronyms + non_sorted_acronyms)\n",
    "annotator.configure(test='t-test_ind', text_format='star', loc='outside',)\n",
    "annotator.configure(comparisons_correction=\"BH\", correction_format=\"replace\")\n",
    "annotator.apply_and_annotate()\n",
    "\n",
    "fig.savefig(os.path.join(analysis_figurepath,f'{figure_key}{pannel_key}.png'),bbox_inches = 'tight',dpi = 216)\n",
    "fig.savefig(os.path.join(analysis_figurepath,f'{figure_key}{pannel_key}.pdf'),bbox_inches = 'tight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36630f18",
   "metadata": {},
   "source": [
    "## Figure E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c9fd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pannel_key = 'E'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d51c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the delta Chronic and Withdrawal effect size\n",
    "delta_effect_size_df = (effect_size_df.set_index('acronym')['Chronic_Morphine'] - effect_size_df.set_index('acronym')['Withdrawal_Morphine']).\\\n",
    "    reset_index().rename(columns = {0:'dcw'}).set_index('acronym')\\\n",
    "        .join(atlas_df.set_index('acronym')[['cleaned_acronym']]).sort_values(by = 'dcw',ascending = False)\n",
    "# add it to the data frame\n",
    "data = delta_effect_size_df.loc[rejected_acronyms,:].join(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4011fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_data = data.sort_values(by = 'label')\n",
    "sorted_acronyms = sorted_data.sort_values(by = ['label','dcw'],ascending = [True,False]).index\n",
    "fig,axs = plt.subplots(len(sub_conditions),1, figsize = (7,len(sub_conditions)*1.0),sharex = True)\n",
    "\n",
    "# calculate variables to plot the heatmatp\n",
    "labels,indexes = np.unique(sorted_data['label'], return_index=True,)\n",
    "ytick_labels = [sorted_data['label'][index] for index in sorted(indexes)[::-1]]\n",
    "\n",
    "\n",
    "counts = [len(np.where(np.array(sorted_data['label']) == ylabel)[0]) for ylabel in ytick_labels][::-1]\n",
    "\n",
    "borders = [0] + list(np.cumsum(counts))\n",
    "\n",
    "x = np.array(borders)\n",
    "yticks = (x[1:] + x[:-1]) / 2\n",
    "\n",
    "for idx,condition in enumerate(sub_conditions):\n",
    "    \n",
    "    subjects = metadf[(metadf.Condition == condition) & (metadf.fname)].ID.values\n",
    "    if idx == len(Conditions)-2:\n",
    "        cbar_ax = fig.add_axes([axs[len(Conditions)-2].get_position().x1+0.01,\n",
    "                                axs[len(Conditions)-2].get_position().y0+0.2,0.05,0.5])\n",
    "        sns.heatmap(data = sorted_data.loc[sorted_acronyms,subjects].T,cbar_ax = cbar_ax\n",
    "        ,ax = axs[idx],vmin = -0., vmax = 2)\n",
    "        cbar_ax.set_ylabel('Scaled density', rotation=270, labelpad=10,fontsize = 10)\n",
    "    else:\n",
    "        sns.heatmap(data = sorted_data.loc[sorted_acronyms,subjects].T,cbar = False\n",
    "        ,ax = axs[idx],vmin =-0., vmax = 2)\n",
    "    axs[idx].set_xticks(yticks)\n",
    "    axs[idx].set_xticklabels(ytick_labels[::-1])\n",
    "    axs[idx].set_xlabel('')\n",
    "    [axs[idx].axvline(border,color = 'yellow',lw = 1,ls = ':') for border in borders[1:-1]]\n",
    "    axs[idx].set_yticks(np.array(range(len(subjects))) + 0.5)\n",
    "    axs[idx].set_yticklabels([],rotation = 0) \n",
    "    axs[idx].set_ylabel(Condition_figure_name[2:4][idx])\n",
    "fig.savefig(os.path.join(analysis_figurepath,f'{figure_key}{pannel_key}_clustered_scaled_heatmap.png'),bbox_inches = 'tight',dpi = 216)\n",
    "fig.savefig(os.path.join(analysis_figurepath,f'{figure_key}{pannel_key}_clustered_scaled_heatmap.pdf'),bbox_inches = 'tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06f6478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e48c837",
   "metadata": {},
   "source": [
    "# Figure 4-supplemental figure 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dc2b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_key = 'Figure4-supplemental figure 1'\n",
    "pannel_key = 'A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68b0b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set paths\n",
    "result_paths = r\"data/lsms/Opioid\"\n",
    "rootpath = r\"G:\\My Drive\\Opioid_whole_brain_manuscript\"\n",
    "metapath = os.path.join(rootpath,\"meta\")\n",
    "analysis_resultpath = os.path.join(rootpath,\"result\")\n",
    "analysis_root_figurepath = os.path.join(rootpath,\"figure\")\n",
    "analysis_figurepath = os.path.join(analysis_root_figurepath,figure_key)\n",
    "for path in [analysis_resultpath,analysis_root_figurepath,analysis_figurepath]:\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c8a201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer labels to sub_merge_df\n",
    "# format the data so it can be plotted\n",
    "sub_merge_df    = merge_df[merge_df.Condition.isin(sub_conditions)]\n",
    "sub_merge_df = sub_merge_df.merge(data.stack().reset_index(drop = False).rename(columns = {'level_1':'ID',0:'scaled_density'}),\n",
    "left_on = ['acronym','ID'],\n",
    "right_on = ['acronym','ID'])\n",
    "sub_merge_df.Condition = sub_merge_df.Condition.astype('str')\n",
    "sub_merge_df = sub_merge_df.reset_index().merge(embedding_df.reset_index(),left_on = 'acronym',right_on = 'acronym')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28a2f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a barplot with a swarmplot overlapped\n",
    "condition1 = 'Saline'\n",
    "sub_effect_size_df = sub_merge_df.loc[(sub_merge_df.acronym.isin(curated_acronyms)) & (sub_merge_df.Condition.isin(sub_conditions)),\\\n",
    "                 ['acronym','Condition','normalized_density']].groupby(['acronym','Condition']).mean().reset_index().\\\n",
    "                    pivot(columns = 'Condition',index = 'acronym',values = 'normalized_density')\n",
    "delta_sub_effect_size_df = (sub_effect_size_df['Chronic_Morphine'] - sub_effect_size_df['Withdrawal_Morphine']).\\\n",
    "    reset_index().rename(columns = {0:'dcw'}).set_index('acronym')\\\n",
    "        .join(atlas_df.set_index('acronym')[['cleaned_acronym']]).sort_values(by = 'dcw',ascending = False)\n",
    "sorted_acronyms_list = []\n",
    "sorted_cleaned_acronyms_list = []\n",
    "nregions_list = []\n",
    "for tlabel in labels:\n",
    "    #print(tlabel)\n",
    "    tacronyms = sub_merge_df[(sub_merge_df.label == tlabel)&(sub_merge_df.Condition.isin(Conditions[1:]))].acronym.unique()\n",
    "    nregions = len(tacronyms)\n",
    "    #a = delta_sub_effect_size_df.set_index('acronym').loc[tacronyms,'Chronic_Morphine']- effect_size_df.set_index('acronym').loc[tacronyms,'Withdrawal_Morphine']\n",
    "    sorted_acronyms = delta_sub_effect_size_df.loc[tacronyms,'dcw'].sort_values(ascending= False).index\n",
    "    sorted_cleaned_acronyms = atlas_df.set_index('acronym').loc[sorted_acronyms,'cleaned_acronym'].values\n",
    "    sorted_cleaned_acronyms_list += list(sorted_cleaned_acronyms)\n",
    "    sorted_acronyms_list += list(sorted_acronyms)\n",
    "    nregions_list += [nregions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d715f095",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig,axs = plt.subplots(1,1,figsize = (len(sorted_acronyms_list)//5,1.5))\n",
    "\n",
    "sub_merge_df.Condition = sub_merge_df.Condition.astype('str')\n",
    "sns.stripplot(data = sub_merge_df[(sub_merge_df.Condition.isin(sub_conditions))],\n",
    "            hue = 'Condition',y = 'normalized_density',x = 'acronym',dodge = True,\n",
    "            order = sorted_acronyms_list,\\\n",
    "        hue_order = Conditions[2:4],ax = axs,palette = Condition_color[2:4],alpha = 0.25,size = 2)\n",
    "sns.pointplot(sub_merge_df[(sub_merge_df.Condition.isin(sub_conditions))],\n",
    "            hue = 'Condition',y = 'normalized_density',x = 'acronym',order = sorted_acronyms_list,\n",
    "            dodge = .5-.5/len(sorted_acronyms_list),\\\n",
    "        hue_order = Conditions[2:4],ax = axs,palette = Condition_color[2:4],\n",
    "markers=\"o\", markersize=3, linestyle=\"none\",linewidth = 0.5)  \n",
    "sns.despine()\n",
    "axs.set_xticklabels(sorted_cleaned_acronyms_list,fontsize = 11,rotation = -45)\n",
    "# remove legend\n",
    "axs.get_legend().remove()\n",
    "axs.set_xlabel('')\n",
    "axs.set_ylabel('Normalized density\\n(cm/mm3)',fontsize = 12)\n",
    "# add a horizontal line to separate the clusters\n",
    "borders = [0] + list(np.cumsum(nregions_list))\n",
    "[axs.axvline(border - 0.5,color = 'gray',lw = 2,ls = ':') for border in borders[1:-1]]\n",
    "\n",
    "\n",
    "pairs = [((c,\"Chronic_Morphine\"),(c,\"Withdrawal_Morphine\")) for c in sorted_acronyms_list]\n",
    "\n",
    "annotator = Annotator(axs, pairs, data=tdata,\n",
    "                     y='scaled_density', x='acronym',hue = 'Condition',\n",
    "                     hue_order = Conditions[2:4],order = sorted_acronyms_list)\n",
    "annotator.configure(test='t-test_ind', text_format='star', loc='outside',)\n",
    "annotator.configure(comparisons_correction=\"BH\", correction_format=\"replace\")\n",
    "annotator.apply_and_annotate()\n",
    "\n",
    "\n",
    "fig.savefig(os.path.join(analysis_figurepath,f'{figure_key}{pannel_key}.png'),bbox_inches = 'tight',dpi = 216)\n",
    "fig.savefig(os.path.join(analysis_figurepath,f'{figure_key}{pannel_key}.pdf'),bbox_inches = 'tight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e293a6d",
   "metadata": {},
   "source": [
    "# Figure 4-supplemental figure 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e3a9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_key = 'Figure4-supplemental figure 2'\n",
    "pannel_key = 'A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d17d8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set paths\n",
    "result_paths = r\"data/lsms/Opioid\"\n",
    "rootpath = r\"G:\\My Drive\\Opioid_whole_brain_manuscript\"\n",
    "metapath = os.path.join(rootpath,\"meta\")\n",
    "analysis_resultpath = os.path.join(rootpath,\"result\")\n",
    "analysis_root_figurepath = os.path.join(rootpath,\"figure\")\n",
    "analysis_figurepath = os.path.join(analysis_root_figurepath,figure_key)\n",
    "for path in [analysis_resultpath,analysis_root_figurepath,analysis_figurepath]:\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08998032",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_merge_df.Condition.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f5d5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer labels to sub_merge_df\n",
    "# format the data so it can be plotted\n",
    "sub_merge_df    = merge_df[merge_df.Condition.isin(Conditions)]\n",
    "sub_merge_df.Condition = sub_merge_df.Condition.astype('str')\n",
    "sub_merge_df = sub_merge_df.reset_index().merge(embedding_df.reset_index(),left_on = 'acronym',right_on = 'acronym')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f716d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition1 = 'Saline'\n",
    "for cidx,condition2 in enumerate(Conditions):\n",
    "    if condition2 == 'Saline':\n",
    "        continue\n",
    "    fig,axs = plt.subplots(1,1,figsize = (len(sorted_acronyms_list)//5,1.5))\n",
    "\n",
    "    sub_merge_df.Condition = sub_merge_df.Condition.astype('str')\n",
    "    sns.stripplot(data = sub_merge_df[(sub_merge_df.Condition.isin([condition2]))],\n",
    "                hue = 'Condition',y = 'normalized_density',x = 'acronym',\n",
    "                order = sorted_acronyms_list,\\\n",
    "                hue_order = [condition2],ax = axs,\\\n",
    "                palette = np.array(Condition_color)[[cidx]],alpha = 0.25,size = 2)\n",
    "    sns.pointplot(sub_merge_df[(sub_merge_df.Condition.isin([condition2]))],\n",
    "                hue = 'Condition',y = 'normalized_density',x = 'acronym',order = sorted_acronyms_list,\n",
    "            hue_order = [condition2],ax = axs,palette = np.array(Condition_color)[[cidx]],\n",
    "    markers=\"o\", markersize=3, linestyle=\"none\",linewidth = 0.5)  \n",
    "    sns.despine()\n",
    "    axs.set_xticklabels(sorted_cleaned_acronyms_list,fontsize = 11,rotation = -45)\n",
    "    # remove legend\n",
    "    axs.get_legend().remove()\n",
    "    axs.set_xlabel('')\n",
    "    axs.set_ylabel('Normalized density',fontsize = 12)\n",
    "    # add a horizontal line to separate the clusters\n",
    "    borders = [0] + list(np.cumsum(nregions_list))\n",
    "    [axs.axvline(border - 0.5,color = 'gray',lw = 2,ls = ':') for border in borders[1:-1]]\n",
    "\n",
    "    \n",
    "    from statannotations.Annotator import Annotator\n",
    "    pairs = [((c,condition1),(c,condition2)) for c in sorted_acronyms_list]\n",
    "\n",
    "    annotator = Annotator(axs, pairs, data = sub_merge_df[(sub_merge_df.Condition.isin([condition1,condition2]))],\n",
    "                hue = 'Condition',y = 'normalized_density',x = 'acronym',\n",
    "                order = sorted_acronyms_list)\n",
    "    annotator.configure(test='t-test_ind', text_format='star', loc='outside',)\n",
    "    annotator.configure(comparisons_correction=\"BH\", correction_format=\"replace\")\n",
    "    annotator.apply_and_annotate()\n",
    "\n",
    "\n",
    "    fig.savefig(os.path.join(analysis_figurepath,f'{figure_key}{pannel_key}_{condition2}.png'),bbox_inches = 'tight',dpi = 216)\n",
    "    fig.savefig(os.path.join(analysis_figurepath,f'{figure_key}{pannel_key}_{condition2}.pdf'),bbox_inches = 'tight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8171d05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scanpy_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
